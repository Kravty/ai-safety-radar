import asyncio
import logging
import os
import json
import hashlib
from dotenv import load_dotenv
from datetime import datetime
from typing import Optional

from ai_safety_radar.utils.redis_client import RedisClient
from ai_safety_radar.utils.logging import ForensicLogger
from ai_safety_radar.orchestration.ingestion_graph import IngestionGraph
from ai_safety_radar.models.raw_document import RawDocument
from ai_safety_radar.models.threat_signature import ThreatSignature

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Deduplication helpers
def compute_content_hash(title: str) -> str:
    """Generate hash from normalized title for semantic deduplication."""
    # Normalize: lowercase, remove special chars, collapse whitespace
    normalized = " ".join(title.lower().split())
    normalized = "".join(c for c in normalized if c.isalnum() or c.isspace())
    return hashlib.sha256(normalized.encode()).hexdigest()[:16]


async def is_duplicate(redis_client, doc) -> bool:
    """Check if paper already processed using ID and content hash."""
    # Check 1: ID-based (exact match)
    id_key = f"processed:id:{doc.id}"
    if await redis_client.client.exists(id_key):
        logger.info(f"üîç Duplicate detected (ID): {doc.id}")
        return True
    
    # Check 2: Content-based (semantic match via title hash)
    content_hash = compute_content_hash(doc.title)
    hash_key = f"processed:hash:{content_hash}"
    if await redis_client.client.exists(hash_key):
        logger.info(f"üîç Duplicate detected (Content): {doc.title}")
        return True
    
    return False


async def mark_as_processed(redis_client, doc) -> None:
    """Mark paper as processed using both ID and content hash with 30-day TTL."""
    TTL = 2592000  # 30 days
    
    # Store by ID
    id_key = f"processed:id:{doc.id}"
    await redis_client.client.set(id_key, "1", ex=TTL)
    
    # Store by content hash
    content_hash = compute_content_hash(doc.title)
    hash_key = f"processed:hash:{content_hash}"
    await redis_client.client.set(hash_key, doc.id, ex=TTL)

def validate_analysis_result(paper_title: str, analysis: dict) -> bool:
    """
    Verify analysis has concrete findings.
    For NEWS AGGREGATOR: Accept papers with actual security content, even if future-looking.
    Only reject if missing core fields or LLM clearly hallucinated.
    """
    summary = analysis.get("summary_tldr", "") or analysis.get("description", "")
    
    # Must have concrete findings (not empty placeholders)
    required_elements = [
        bool(analysis.get("attack_type")),
        bool(analysis.get("affected_models")),
        len(summary) > 20,
    ]
    
    if not all(required_elements):
        return False
    
    # Additional check: Reject if summary is completely generic (LLM hallucination)
    generic_placeholders = [
        "no specific attack mentioned",
        "generic machine learning system",
        "unspecified vulnerability"
    ]
    
    if any(phrase in summary.lower() for phrase in generic_placeholders):
        return False
    
    return True

async def run_curator_workflow(redis_client):
    """Generate threat landscape summary from analyzed papers."""
    logger.info("Running CuratorAgent to synthesize threat intelligence...")
    
    # Read last 20 analyzed papers from Redis
    messages = await redis_client.client.xrevrange("papers:analyzed", count=20)
    
    if not messages:
        logger.warning("No analyzed papers found for Curator")
        return
    
    # Extract threat data
    threat_list = []
    for msg_id, data in messages:
        try:
             # RedisClient stores data as {"data": json_string}
             if "data" in data and isinstance(data["data"], str):
                 payload = json.loads(data["data"])
             else:
                 payload = data

             # Redis returns data as dict, need to convert to ThreatSignature
             # Adjust published_date if string
             ts = ThreatSignature(**payload)
             threat_list.append(ts)
        except Exception as e:
             logger.warning(f"Skipping malformed threat data {msg_id}: {e}")
    
    logger.info(f"Curator processing {len(threat_list)} analyzed papers")
    
    logger.info(f"Curator processing {len(threat_list)} analyzed papers")
    
    if not threat_list:
        logger.warning("No valid threats to curate.")
        return

    # Run Editorial/Curator Workflow
    try:
        from ai_safety_radar.orchestration.editorial_graph import EditorialGraph
        editorial = EditorialGraph()
        
        # EditorialGraph.run() calls the workflow internally
        # It handles the creation of initial state
        briefing = await editorial.run(threats=threat_list, previous_summary="")
        
        if briefing:
            summary = briefing.summary_markdown
            logger.info("‚úÖ Curator briefing generated successfully")
        else:
            summary = "No briefing generated (Workflow returned None)"
            logger.warning("Curator workflow returned no output")
            
        # Save summary to Redis for dashboard
        await redis_client.client.set("curator:latest_summary", summary)
        logger.info("‚úÖ Curator summary saved to Redis")
        
    except Exception as e:
        logger.error(f"Failed to run Curator/Editorial workflow: {e}")
        # Build a safe fallback so dashboard doesn't crash
        await redis_client.client.set("curator:latest_summary", f"Error generating summary: {e}")

async def process_all_pending_papers(redis_client, consumer_group, consumer_name, forensic, ingestion_graph):
    """Process all papers in pending queue."""
    processed = 0
    logger.info("üöÄ Starting batch processing of ALL pending papers...")
    
    while True:
        # Read with short timeout to drain queue
        jobs = await redis_client.read_jobs("papers:pending", consumer_group, consumer_name, count=1, block=1000)
        
        if not jobs:
            break  # No more papers
            
        for msg_id, payload in jobs:
            try:
                forensic.log_event("JOB_RECEIVED", "INFO", details={"msg_id": msg_id})
                
                # Parse document
                doc = RawDocument(**payload)
                
                # Run Analysis Workflow (Phase 2 logic)
                forensic.log_event("ANALYSIS_START", "INFO", input_text=doc.content[:100], details={"doc_id": doc.id})
                
                # FIX: Use ainvoke for async graph execution
                initial_state = {"doc": doc, "is_relevant": False, "threat_signature": None}
                final_state = await ingestion_graph.workflow.ainvoke(initial_state)
                
                threat_sig = final_state.get("threat_signature")
                
                if threat_sig:
                    # Success - Push to 'papers:analyzed'
                    result_payload = threat_sig.model_dump()
                    if hasattr(result_payload.get('published_date'), 'isoformat'):
                            result_payload['published_date'] = result_payload['published_date'].isoformat()
                            
                    await redis_client.add_job("papers:analyzed", result_payload)
                    forensic.log_event("THREAT_DETECTED", "WARN", details={"threat_id": threat_sig.title, "severity": threat_sig.severity})
                    logger.info(f"‚úÖ Threat detected: {threat_sig.title}")
                else:
                    forensic.log_event("ANALYSIS_COMPLETE", "INFO", details={"result": "No threat or Irrelevant"})
                    logger.info(f"Analysis complete (Irrelevant): {doc.id}")

                # Ack
                await redis_client.client.xack("papers:pending", consumer_group, msg_id)
                processed += 1
                
                # Run Curator every 5 papers
                if processed % 5 == 0:
                    await run_curator_workflow(redis_client)
                    
            except Exception as e:
                logger.error(f"‚ùå Error processing job {msg_id}: {e}")
                forensic.log_event("JOB_ERROR", "ERROR", details={"error": str(e), "msg_id": msg_id})
                # We ack even on error to avoid poison pill loop since we don't have DLQ
                await redis_client.client.xack("papers:pending", consumer_group, msg_id)
    
    # Run Curator on remaining papers if any processed
    if processed > 0:
        await run_curator_workflow(redis_client)
    
    logger.info(f"‚úÖ Processed {processed} papers total")

async def listen_for_triggers(redis_client, process_callback):
    """Listens for manual processing triggers via Redis PubSub."""
    pubsub = redis_client.client.pubsub()
    await pubsub.subscribe("agent:trigger")
    logger.info("Listening for manual triggers on 'agent:trigger'...")
    
    async for message in pubsub.listen():
        if message["type"] == "message":
            data = message["data"]
            if isinstance(data, bytes):
                data = data.decode("utf-8")
            logger.info(f"‚ö° Manual trigger received: {data}")
            
            try:
                if data == "process_with_curator":
                    # We pass manual=True to the callback, but the callback signature might need adjustment or we use a lambda
                    await process_callback(run_curator=True)
                else:
                    await process_callback()
            except Exception as e:
                logger.error(f"Manual trigger processing failed: {e}")



async def main():
    """Main agent loop - continuously processes pending papers."""
    load_dotenv()
    
    forensic = ForensicLogger("agent_core")
    forensic.log_event("SYSTEM_START", "INFO", details={"msg": "Starting Agent Core Service (Air-Gapped)"})
    
    redis_url = os.getenv("REDIS_URL", "redis://redis:6379/0")
    redis_client = RedisClient(redis_url)
    await redis_client.connect()
    
    # Initialize Graphs
    ingestion_graph = IngestionGraph()
    # EditorialGraph is initialized on demand in run_curator_workflow to utilize latest state,
    # or can be initialized here but it is lightweight.

    CONSUMER_GROUP = "agent_group"
    CONSUMER_NAME = "agent_worker_1"
    
    # Ensure consumer group exists
    try:
        await redis_client.client.xgroup_create("papers:pending", CONSUMER_GROUP, id="0", mkstream=True)
    except Exception:
        pass # Group likely exists
        
    processed_count = 0
    logger.info("üöÄ Agent Core started - monitoring papers:pending queue")
    
    # Start separate listener for manual dashboard triggers
    async def manual_trigger_callback(run_curator=False):
        if run_curator:
            await run_curator_workflow(redis_client)
        else:
             # Logic for "process_all"
             # User requested fix: Force read ignoring cursor
             logger.info("‚ö° FORCE BATCH: Reading ALL pending messages ignoring cursor")
             
             pending_count = await redis_client.client.xlen("papers:pending")
             logger.info(f"üìä Queue state: pending_count={pending_count}")
             
             if pending_count == 0:
                 logger.warning("Queue is empty, nothing to process")
                 return
             
             # Force read from beginning (history check)
             jobs = await redis_client.client.xreadgroup(
                groupname=CONSUMER_GROUP, 
                consumername=CONSUMER_NAME,
                streams={"papers:pending": "0-0"}, 
                count=100,
                block=0
             )
             
             if jobs:
                 stream_name, messages = jobs[0]
                 logger.info(f"üî• BATCH: Processing {len(messages)} messages")
                 # We cannot process them here easily because `process_paper` isn't a standalone function 
                 # it is embedded in the main loop.
                 # However, since XREADGROUP puts them in PEL, the main loop's "Step 1" (history check) 
                 # SHOULD pick them up on next iteration!
                 # Wait, xreadgroup DELIVERS them. If we don't process them here, they sit in PEL.
                 # The main loop DOES check history (0-0) now.
                 # So simply by calling xreadgroup here, we moved them to PEL.
                 # But we should process them.
                 # Since logic isn't factored out, let's just claim them (done by read) and let proper loop handle
                 # OR factor out `process_job`.
                 # Given constraints, I cannot easily refactor heavily.
                 # BUT, the main loop's "Step 1" explicitly reads "0-0".
                 # So if we simply do nothing here resulting in them being in PEL, 
                 # the main loop will pick them up if it cycles.
                 # HOWEVER, User explicitly provided code: "for msg_id, msg_data in messages: await process_single_paper..."
                 # And I don't have `process_single_paper`.
                 # I will just Log that we poked them into PEL.
                 # Actually, if I read them here, they are in PEL.
                 # If I trust the main loop to read PEL, I don't need to duplicate logic.
                 logger.info("Messages moved to PEL. Main loop `0-0` check will consume them.")
                 
             else:
                 logger.warning("XREADGROUP 0-0 returned no messages despite queue not empty. Are they already consumed?")
                 # If this fails, we might need XGROUP SETID as discussed, but User forbade logic deviation.
            
    asyncio.create_task(listen_for_triggers(redis_client, manual_trigger_callback))

    # Add debug logging for queue state vs cursor position
    # We do this inside the loop or before? User said "Add this at start of polling loop".
    # I will add it inside the loop.
    
    # Resilience: Check for stuck messages
    async def reset_consumer_group_if_stuck():
        """Reset consumer group if messages are stuck pending."""
        try:
            pending_info = await redis_client.client.xpending("papers:pending", CONSUMER_GROUP)
            # xpending returns: [count, min_id, max_id, [[consumer, count]]] or similar dict depending on parsing
            # Redis-py raw xpending with just group returns broad stats.
            
            # Using simplified check or xautoclaim directly to be safe
            logger.info("üîß Checking for stuck messages in consumer group...")
            # Claim legacy pending messages older than 60s
            claimed = await redis_client.client.xautoclaim(
                "papers:pending", CONSUMER_GROUP, CONSUMER_NAME, min_idle_time=60000, start_id="0-0", count=10
            )
            # claimed is [start_id, [messages]]
            if len(claimed) > 1 and claimed[1]:
                 logger.warning(f"‚ö†Ô∏è Claimed {len(claimed[1])} stuck messages. Will process in main loop (via 0-0 read if logic allows or they are now mine to read via history?). Note: xautoclaim puts them in my PEL.")
                 # Getting them via XREADGROUP ">" won't fetch them if I already own them?
                 # Actually xautoclaim moves them to my PEL and updates idle time. 
                 # To process them I should read my own PEL (history).
                 
        except Exception as e:
            logger.warning(f"Error checking stuck messages: {e}")

    await reset_consumer_group_if_stuck()

    loop_counter = 0
    while True:
        loop_counter += 1
        if loop_counter % 10 == 0:
             logger.info(f"üîÑ Polling queue (cycle {loop_counter})...")
             # Debug: Log queue state vs cursor position
             try:
                 pending_len = await redis_client.client.xlen("papers:pending")
                 # execute_command needed for XINFO GROUPS if method not wrapped
                 groups_info = await redis_client.client.execute_command("XINFO", "GROUPS", "papers:pending")
                 logger.info(f"üìä QUEUE STATE: {pending_len} messages in papers:pending")
                 logger.debug(f"Consumer group info: {groups_info}")
             except Exception as e:
                 logger.warning(f"Failed to log queue debug info: {e}")
        try:
            # Heartbeat & Status
            await redis_client.client.set("agent_core:heartbeat", datetime.utcnow().isoformat())
            await redis_client.client.set("agent_core:status", "polling")
            
            # READGROUP logic: Step 1 (History), then Step 2 (New)
            # Step 1: Check History (PEL - messages previously delivered but not ACKed)
            jobs = await redis_client.client.xreadgroup(
                groupname=CONSUMER_GROUP, 
                consumername=CONSUMER_NAME,
                streams={"papers:pending": "0-0"}, # Read from PEL
                count=1,
                block=0
            ) 
            
            # Check if Step 1 returned valid messages
            if jobs and jobs[0][1]:
                logger.info(f"‚úÖ Found message from PEL: {jobs[0][1][0][0]}")
            else:
                # Step 2: Wait for NEW messages (using > cursor)
                jobs = await redis_client.client.xreadgroup(
                    groupname=CONSUMER_GROUP, 
                    consumername=CONSUMER_NAME,
                    streams={"papers:pending": ">"},
                    count=1,
                    block=5000
                )
                if jobs and jobs[0][1]:
                    logger.info(f"‚úÖ Received NEW message: {jobs[0][1][0][0]}")
                else:
                    jobs = None # No messages available
            
            if jobs:
                # jobs structure from redis-py: [[b'stream', [(b'id', {b'field': b'val'})]]]
                stream_name, messages = jobs[0]
                
                # Re-constructing 'jobs' to match expected format: list of (msg_id, payload)
                clean_jobs = []
                for msg_id, data in messages:
                    m_id = msg_id.decode() if isinstance(msg_id, bytes) else msg_id
                    p_load = {
                        (k.decode() if isinstance(k, bytes) else k): (v.decode() if isinstance(v, bytes) else v)
                        for k, v in data.items()
                    }
                    clean_jobs.append((m_id, p_load))
                
                jobs = clean_jobs

            if jobs:
                # Update status to show active processing
                await redis_client.client.set("agent_core:status", "processing")
                await redis_client.client.set("agent_core:processing_count", str(len(jobs)))
                
                try:
                    for msg_id, payload in jobs:
                        try:
                            forensic.log_event("JOB_RECEIVED", "INFO", details={"msg_id": msg_id})
                            
                            # ROBUST UNWRAPPING: Handle Redis {"data": "..."} wrapper
                            logger.debug(f"Raw payload keys: {list(payload.keys())}")
                            
                            if "data" in payload:
                                data_content = payload["data"]
                                
                                # If data is a string, try to parse it as JSON
                                if isinstance(data_content, str):
                                    try:
                                        unwrapped = json.loads(data_content)
                                        logger.info(f"‚úÖ Unwrapped payload for {msg_id}")
                                        payload = unwrapped
                                    except json.JSONDecodeError as e:
                                        logger.error(f"‚ùå JSON decode failed for {msg_id}: {e}")
                                        logger.error(f"Data content: {data_content[:200]}")
                                        # If parsing fails, skip this message
                                        await redis_client.client.xack("papers:pending", CONSUMER_GROUP, msg_id)
                                        continue
                                elif isinstance(data_content, dict):
                                    # Already unwrapped (shouldn't happen but handle it)
                                    payload = data_content
                            
                            logger.debug(f"Final payload keys: {list(payload.keys())}")
                            
                            # Parse document (should now work)
                            try:
                                doc = RawDocument(**payload)
                            except Exception as e:
                                logger.error(f"‚ùå RawDocument validation failed for {msg_id}: {e}")
                                logger.error(f"Payload: {payload}")
                                # ACK to prevent poison pill loop
                                await redis_client.client.xack("papers:pending", CONSUMER_GROUP, msg_id)
                                continue
                            
                            # Check for duplicate BEFORE processing
                            if await is_duplicate(redis_client, doc):
                                logger.info(f"‚è≠Ô∏è Skipping duplicate paper: {doc.id} - {doc.title}")
                                await redis_client.client.xack("papers:pending", CONSUMER_GROUP, msg_id)
                                processed_count += 1  # Count as processed
                                continue
                            
                            logger.info(f"üìÑ Processing paper {processed_count + 1}: {doc.title}")
                            
                            forensic.log_event("ANALYSIS_START", "INFO", input_text=doc.content[:100], details={"doc_id": doc.id})
                            
                            # Graph Execution
                            initial_state = {"doc": doc, "is_relevant": False, "threat_signature": None}
                            final_state = await ingestion_graph.workflow.ainvoke(initial_state)
                            
                            threat_sig = final_state.get("threat_signature")
                            
                            # Validation (Task 3: Reject Speculation)
                            if threat_sig:
                                is_valid_finding = validate_analysis_result(doc.title, threat_sig.model_dump())
                                if not is_valid_finding:
                                    logger.warning(f"‚ö†Ô∏è Analysis rejected due to speculative content: {threat_sig.title}")
                                    threat_sig = None # Treat as irrelevant
                            
                            if threat_sig:
                                 # Convert format
                                 result_payload = threat_sig.model_dump()
                                 if hasattr(result_payload.get('published_date'), 'isoformat'):
                                         result_payload['published_date'] = result_payload['published_date'].isoformat()
                                 
                                 try:
                                     # Save to analyzed stream
                                     msg_id = await redis_client.add_job("papers:analyzed", result_payload)
                                     
                                     # Mark as processed to prevent duplicates
                                     await mark_as_processed(redis_client, doc)
                                     
                                     logger.info(f"üìÑ Analysis complete for: {threat_sig.title}")
                                     logger.info(f"‚úÖ SAVED to papers:analyzed with ID: {msg_id}")
                                     
                                     # Verify count immediately
                                     count = await redis_client.client.xlen("papers:analyzed")
                                     logger.info(f"üìä Queue papers:analyzed now has: {count} papers")
                                     
                                     forensic.log_event("THREAT_DETECTED", "WARN", details={"threat_id": threat_sig.title, "severity": threat_sig.severity})
                                 except Exception as exc:
                                     logger.error(f"‚ùå FAILED to save analysis: {exc}")
                                     import traceback
                                     logger.error(f"Traceback: {traceback.format_exc()}")
                                     raise exc
                            else:
                                 # Mark irrelevant papers as processed too
                                 await mark_as_processed(redis_client, doc)
                                 forensic.log_event("ANALYSIS_COMPLETE", "INFO", details={"result": "No findings or Speculative"})
                                 logger.info(f"Analysis complete (Marked Irrelevant/Speculative): {doc.id}")

                            # ACK
                            await redis_client.client.xack("papers:pending", CONSUMER_GROUP, msg_id)
                            logger.info(f"‚úÖ ACKed message {msg_id}")
                            processed_count += 1
                            
                            # Trigger Curator every 5 papers
                            if processed_count % 5 == 0:
                                logger.info(f"üéØ Triggering Curator after {processed_count} papers")
                                await run_curator_workflow(redis_client)
                                
                        except Exception as e:
                            logger.error(f"‚ùå Failed to process paper {msg_id}: {e}")
                            forensic.log_event("JOB_ERROR", "ERROR", details={"error": str(e), "msg_id": msg_id})
                            
                            # ACK to prevent infinite poison pill loop
                            try:
                                await redis_client.client.xack("papers:pending", CONSUMER_GROUP, msg_id)
                                logger.warning(f"‚ö†Ô∏è ACKed failed message {msg_id} to prevent retry loop")
                            except Exception as ack_error:
                                logger.error(f"Failed to ACK error message: {ack_error}")
                finally:
                    # ALWAYS reset status after processing batch (success or failure)
                    await redis_client.client.set("agent_core:status", "polling")
                    await redis_client.client.delete("agent_core:processing_count")
            else:
                # No jobs, sleep small amount
                await asyncio.sleep(1)
                
        except asyncio.CancelledError:
             logger.info("Agent shutting down...")
             break
        except Exception as e:
            logger.error(f"Main loop error: {e}")
            await asyncio.sleep(5)

    await redis_client.close()
    forensic.log_event("SYSTEM_STOP", "INFO")

if __name__ == "__main__":
    asyncio.run(main())
