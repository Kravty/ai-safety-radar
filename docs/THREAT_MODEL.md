# Threat Model â€” AI Safety Radar

## Overview

This document defines the security boundaries, threats, and mitigations for the AI Safety Radar system.

## Trust Boundaries

```mermaid
flowchart TB
    subgraph Internet["ðŸŒ Internet (Untrusted)"]
        ArXiv[(ArXiv API)]
        OpenAI[OpenAI API]
    end

    subgraph Host["ðŸ–¥ï¸ Host OS (Trusted)"]
        subgraph Secrets["ðŸ” Podman Secrets"]
            APIKey[openai_api_key]
        end

        subgraph PublicNet["public_io_net"]
            Ingestion[ingestion_service]
        end

        subgraph InternalNet["internal_msg_net (No Internet)"]
            Redis[(Redis)]
            Agent[agent_core]
            Dashboard[dashboard]
        end
    end

    ArXiv -->|HTTPS| Ingestion
    Ingestion -->|read| APIKey
    Ingestion <-->|streams| Redis
    Agent -->|read| APIKey
    Agent <-->|HTTPS| OpenAI
    Agent <-->|streams| Redis
    Redis <-->|read| Dashboard

    style Internet fill:#ffebee
    style Secrets fill:#e8f5e9
    style InternalNet fill:#e3f2fd
```

## Assets

| Asset | Sensitivity | Location |
|-------|-------------|----------|
| OpenAI API Key | **Critical** | `/run/secrets/openai_api_key` |
| Analyzed Papers | Medium | Redis `papers:analyzed` |
| Audit Logs | Medium | `./logs/audit.jsonl` |
| User Trust | High | Dashboard integrity |

## Threats & Mitigations

### T1: API Key Exposure

**Threat:** OpenAI API key leaked via logs, env vars, or compose files.

**Impact:** Critical â€” Unauthorized API usage, billing fraud.

**Mitigations:**
- âœ… Key stored as Podman secret, mounted at `/run/secrets/`
- âœ… Never in environment variables or docker-compose.yml
- âœ… `secrets.py` reads file directly, never logs value
- âœ… `.gitignore` excludes secret files

**Residual Risk:** Low â€” Host compromise could access secret file.

### T2: Prompt Injection via ArXiv Paper

**Threat:** Malicious paper contains prompt injection payload designed to:
- Exfiltrate data
- Execute arbitrary code
- Manipulate analysis output

**Impact:** High â€” Data exfiltration or system compromise.

**Mitigations:**
- âœ… `agent_core` on internal network (no direct internet)
- âœ… Containers run as non-root (uid 1000)
- âœ… All capabilities dropped (`cap_drop: ALL`)
- âœ… `no-new-privileges` security option
- âœ… Pydantic validation rejects malformed outputs
- âœ… Input hashed and logged for forensic analysis

**Residual Risk:** Medium â€” Sophisticated injection could still manipulate outputs.

### T3: Supply Chain Compromise

**Threat:** Malicious dependency introduced via PyPI or container image.

**Impact:** Critical â€” Full system compromise.

**Mitigations:**
- âœ… `uv.lock` pins exact dependency versions
- âœ… Base image pinned (`python:3.11-slim-bookworm`)
- âœ… Network isolation limits C2 communication
- âœ… Containers have minimal filesystem access

**Residual Risk:** Medium â€” Zero-day in pinned dependency.

### T4: Redis Data Loss / Consumer Group Corruption

**Threat:** `FLUSHDB` or stream deletion breaks consumer groups.

**Impact:** Medium â€” Papers lost, processing halts.

**Mitigations:**
- âœ… Documentation explicitly warns against `FLUSHDB`
- âœ… Safe reset procedure documented:
  ```bash
  redis-cli DEL papers:pending papers:analyzed
  redis-cli XGROUP CREATE papers:pending agent_group 0 MKSTREAM
  ```
- âœ… Redis AOF persistence enabled
- âœ… Volume mount preserves data across restarts

**Residual Risk:** Low â€” Operator error still possible.

### T5: Log Leakage

**Threat:** Sensitive content (paper abstracts, prompts) leaked via logs.

**Impact:** Low â€” Privacy concern, not security breach.

**Mitigations:**
- âœ… Prompts hashed (SHA256) before logging
- âœ… API keys never logged
- âœ… Structured JSON logging (no string interpolation accidents)
- âœ… Log files excluded from git

**Residual Risk:** Low â€” Abstracts are public anyway.

### T6: Dashboard Unauthorized Access

**Threat:** Unauthorized users access dashboard and view analyzed papers.

**Impact:** Low â€” Data is derived from public ArXiv papers.

**Mitigations:**
- âš ï¸ Dashboard currently has no authentication
- âœ… Bind to localhost only by default
- âœ… Internal network isolation

**Residual Risk:** Medium â€” Should add auth for production deployment.

### T7: Denial of Service

**Threat:** Attacker floods system with papers or malformed requests.

**Impact:** Medium â€” Service unavailability.

**Mitigations:**
- âœ… Rate limiting via `interval_seconds` config
- âœ… `max_results` caps papers per cycle
- âœ… Redis memory limits possible via config
- âš ï¸ No explicit rate limiting on API calls

**Residual Risk:** Medium â€” OpenAI rate limits provide some protection.

## Data Flow Diagram

```mermaid
flowchart LR
    subgraph Ingestion
        A1[Fetch Papers] --> A2[Regex Filter]
        A2 --> A3{Score?}
        A3 -->|< 25| A4[Reject]
        A3 -->|25-65| A5[LLM Filter]
        A3 -->|> 65| A6[Accept]
        A5 --> A6
        A5 --> A4
    end

    subgraph Processing
        A6 --> B1[(papers:pending)]
        B1 --> B2[ExtractionAgent]
        B2 --> B3[CriticAgent]
        B3 --> B4[(papers:analyzed)]
    end

    subgraph Output
        B4 --> C1[Dashboard]
        B4 --> C2[Curator]
    end

    style A4 fill:#ffcdd2
    style A6 fill:#c8e6c9
```

## Security Controls Summary

| Control | Status | Notes |
|---------|--------|-------|
| Secrets via Podman | âœ… | Not env vars |
| Non-root containers | âœ… | uid 1000 |
| Capability drop | âœ… | cap_drop: ALL |
| Network isolation | âœ… | Internal networks |
| Input validation | âœ… | Pydantic models |
| Audit logging | âœ… | Hashed prompts |
| Dependency pinning | âœ… | uv.lock |
| Authentication | âš ï¸ | Not implemented |
| Rate limiting | âš ï¸ | Config-based only |

## Monitoring Recommendations

1. **API Usage** â€” Monitor OpenAI billing dashboard
2. **Queue Growth** â€” Alert if `papers:pending` > 1000
3. **Error Rate** â€” Monitor `LLM_RESPONSE status=error` in logs
4. **Container Health** â€” Podman healthchecks

## Incident Response

### API Key Compromise

1. Revoke key immediately in OpenAI dashboard
2. Create new key: `echo "sk-new" | podman secret rm openai_api_key && podman secret create openai_api_key -`
3. Restart containers: `podman-compose down && podman-compose up -d`
4. Review audit logs for unauthorized usage

### Data Corruption

1. Stop processing: `podman-compose stop agent_core`
2. Backup Redis: `podman exec redis redis-cli BGSAVE`
3. Investigate logs
4. Safe reset if needed (see T4 mitigation)
5. Re-run backfill

## References

- [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
- [Docker Security Best Practices](https://docs.docker.com/develop/security-best-practices/)
- [Redis Security](https://redis.io/docs/latest/operate/rs/security/)
